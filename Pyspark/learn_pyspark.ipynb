{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMnpdGb3dwJO/gSxgG8U9EK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5DXSytoPiCV","executionInfo":{"status":"ok","timestamp":1687501882193,"user_tz":-420,"elapsed":57495,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"e90bf02d-f428-47d0-a5ee-57c7b3ffb10e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=466869008038145201e5cb2f4b8a9e7a07d0bc2e53f622286b83905fee637346\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]}]},{"cell_type":"markdown","source":["# Aggregate"],"metadata":{"id":"5a0NU2RpPGHI"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmfDNAeFO5Uh","executionInfo":{"status":"ok","timestamp":1687490134202,"user_tz":-420,"elapsed":41817,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"c9b9cee3-3da1-49af-c022-fb926f231cb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- employee_name: string (nullable = true)\n"," |-- department: string (nullable = true)\n"," |-- salary: long (nullable = true)\n","\n","+-------------+----------+------+\n","|employee_name|department|salary|\n","+-------------+----------+------+\n","|James        |Sales     |3000  |\n","|Michael      |Sales     |4600  |\n","|Robert       |Sales     |4100  |\n","|Maria        |Finance   |3000  |\n","|James        |Sales     |3000  |\n","|Scott        |Finance   |3300  |\n","|Jen          |Finance   |3900  |\n","|Jeff         |Marketing |3000  |\n","|Kumar        |Marketing |2000  |\n","|Saif         |Sales     |4100  |\n","+-------------+----------+------+\n","\n","approx_count_distinct: 6\n","avg: 3400.0\n","+------------------------------------------------------------+\n","|collect_list(salary)                                        |\n","+------------------------------------------------------------+\n","|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n","+------------------------------------------------------------+\n","\n","+------------------------------------+\n","|collect_set(salary)                 |\n","+------------------------------------+\n","|[4600, 3000, 3900, 4100, 3300, 2000]|\n","+------------------------------------+\n","\n","+----------------------------------+\n","|count(DISTINCT department, salary)|\n","+----------------------------------+\n","|8                                 |\n","+----------------------------------+\n","\n","Distinct Count of Department & Salary: 8\n","count: Row(count(salary)=10)\n","+-------------+\n","|first(salary)|\n","+-------------+\n","|3000         |\n","+-------------+\n","\n","+------------+\n","|last(salary)|\n","+------------+\n","|4100        |\n","+------------+\n","\n","+-------------------+\n","|kurtosis(salary)   |\n","+-------------------+\n","|-0.6467803030303032|\n","+-------------------+\n","\n","+-----------+\n","|max(salary)|\n","+-----------+\n","|4600       |\n","+-----------+\n","\n","+-----------+\n","|min(salary)|\n","+-----------+\n","|2000       |\n","+-----------+\n","\n","+-----------+\n","|avg(salary)|\n","+-----------+\n","|3400.0     |\n","+-----------+\n","\n","+--------------------+\n","|skewness(salary)    |\n","+--------------------+\n","|-0.12041791181069571|\n","+--------------------+\n","\n","+-------------------+-------------------+------------------+\n","|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n","+-------------------+-------------------+------------------+\n","|765.9416862050705  |765.9416862050705  |726.636084983398  |\n","+-------------------+-------------------+------------------+\n","\n","+-----------+\n","|sum(salary)|\n","+-----------+\n","|34000      |\n","+-----------+\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pyspark/sql/functions.py:752: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n","  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["+--------------------+\n","|sum(DISTINCT salary)|\n","+--------------------+\n","|20900               |\n","+--------------------+\n","\n","+-----------------+-----------------+---------------+\n","|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n","+-----------------+-----------------+---------------+\n","|586666.6666666666|586666.6666666666|528000.0       |\n","+-----------------+-----------------+---------------+\n","\n"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import approx_count_distinct,collect_list\n","from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n","from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness\n","from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n","from pyspark.sql.functions import variance,var_samp,  var_pop\n","\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","\n","simpleData = [(\"James\", \"Sales\", 3000),\n","    (\"Michael\", \"Sales\", 4600),\n","    (\"Robert\", \"Sales\", 4100),\n","    (\"Maria\", \"Finance\", 3000),\n","    (\"James\", \"Sales\", 3000),\n","    (\"Scott\", \"Finance\", 3300),\n","    (\"Jen\", \"Finance\", 3900),\n","    (\"Jeff\", \"Marketing\", 3000),\n","    (\"Kumar\", \"Marketing\", 2000),\n","    (\"Saif\", \"Sales\", 4100)\n","  ]\n","schema = [\"employee_name\", \"department\", \"salary\"]\n","\n","df = spark.createDataFrame(data=simpleData, schema = schema)\n","df.printSchema()\n","df.show(truncate=False)\n","\n","print(\"approx_count_distinct: \" + \\\n","      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n","\n","print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n","\n","df.select(collect_list(\"salary\")).show(truncate=False)\n","\n","df.select(collect_set(\"salary\")).show(truncate=False)\n","\n","df2 = df.select(countDistinct(\"department\", \"salary\"))\n","df2.show(truncate=False)\n","print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))\n","\n","print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n","df.select(first(\"salary\")).show(truncate=False)\n","df.select(last(\"salary\")).show(truncate=False)\n","df.select(kurtosis(\"salary\")).show(truncate=False)\n","df.select(max(\"salary\")).show(truncate=False)\n","df.select(min(\"salary\")).show(truncate=False)\n","df.select(mean(\"salary\")).show(truncate=False)\n","df.select(skewness(\"salary\")).show(truncate=False)\n","df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n","    stddev_pop(\"salary\")).show(truncate=False)\n","df.select(sum(\"salary\")).show(truncate=False)\n","df.select(sumDistinct(\"salary\")).show(truncate=False)\n","df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n","  .show(truncate=False)\n"]},{"cell_type":"markdown","source":["#Function"],"metadata":{"id":"lKlzeu1eQRZ8"}},{"cell_type":"code","source":["\n","import pyspark\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","\n","simpleData = ((\"James\", \"Sales\", 3000), \\\n","    (\"Michael\", \"Sales\", 4600),  \\\n","    (\"Robert\", \"Sales\", 4100),   \\\n","    (\"Maria\", \"Finance\", 3000),  \\\n","    (\"James\", \"Sales\", 3000),    \\\n","    (\"Scott\", \"Finance\", 3300),  \\\n","    (\"Jen\", \"Finance\", 3900),    \\\n","    (\"Jeff\", \"Marketing\", 3000), \\\n","    (\"Kumar\", \"Marketing\", 2000),\\\n","    (\"Saif\", \"Sales\", 4100) \\\n","  )\n","\n","columns= [\"employee_name\", \"department\", \"salary\"]\n","\n","df = spark.createDataFrame(data = simpleData, schema = columns)\n","\n","df.printSchema()\n","df.show(truncate=False)\n","\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number\n","windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n","\n","df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n","    .show(truncate=False)\n","\n","from pyspark.sql.functions import rank\n","df.withColumn(\"rank\",rank().over(windowSpec)) \\\n","    .show()\n","\n","from pyspark.sql.functions import dense_rank\n","df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n","    .show()\n","\n","from pyspark.sql.functions import percent_rank\n","df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n","    .show()\n","\n","from pyspark.sql.functions import ntile\n","df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n","    .show()\n","\n","from pyspark.sql.functions import cume_dist\n","df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n","   .show()\n","\n","from pyspark.sql.functions import lag\n","df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n","      .show()\n","\n","from pyspark.sql.functions import lead\n","df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n","    .show()\n","\n","windowSpecAgg  = Window.partitionBy(\"department\")\n","from pyspark.sql.functions import col,avg,sum,min,max,row_number\n","df.withColumn(\"row\",row_number().over(windowSpec)) \\\n","  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n","  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n","  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n","  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n","  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n","  .show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IEHtQzDPhCF","executionInfo":{"status":"ok","timestamp":1687490226424,"user_tz":-420,"elapsed":10158,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"565a79a8-3f01-46d1-eaaa-0bfe56bf58fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- employee_name: string (nullable = true)\n"," |-- department: string (nullable = true)\n"," |-- salary: long (nullable = true)\n","\n","+-------------+----------+------+\n","|employee_name|department|salary|\n","+-------------+----------+------+\n","|James        |Sales     |3000  |\n","|Michael      |Sales     |4600  |\n","|Robert       |Sales     |4100  |\n","|Maria        |Finance   |3000  |\n","|James        |Sales     |3000  |\n","|Scott        |Finance   |3300  |\n","|Jen          |Finance   |3900  |\n","|Jeff         |Marketing |3000  |\n","|Kumar        |Marketing |2000  |\n","|Saif         |Sales     |4100  |\n","+-------------+----------+------+\n","\n","+-------------+----------+------+----------+\n","|employee_name|department|salary|row_number|\n","+-------------+----------+------+----------+\n","|Maria        |Finance   |3000  |1         |\n","|Scott        |Finance   |3300  |2         |\n","|Jen          |Finance   |3900  |3         |\n","|Kumar        |Marketing |2000  |1         |\n","|Jeff         |Marketing |3000  |2         |\n","|James        |Sales     |3000  |1         |\n","|James        |Sales     |3000  |2         |\n","|Robert       |Sales     |4100  |3         |\n","|Saif         |Sales     |4100  |4         |\n","|Michael      |Sales     |4600  |5         |\n","+-------------+----------+------+----------+\n","\n","+-------------+----------+------+----+\n","|employee_name|department|salary|rank|\n","+-------------+----------+------+----+\n","|        Maria|   Finance|  3000|   1|\n","|        Scott|   Finance|  3300|   2|\n","|          Jen|   Finance|  3900|   3|\n","|        Kumar| Marketing|  2000|   1|\n","|         Jeff| Marketing|  3000|   2|\n","|        James|     Sales|  3000|   1|\n","|        James|     Sales|  3000|   1|\n","|       Robert|     Sales|  4100|   3|\n","|         Saif|     Sales|  4100|   3|\n","|      Michael|     Sales|  4600|   5|\n","+-------------+----------+------+----+\n","\n","+-------------+----------+------+----------+\n","|employee_name|department|salary|dense_rank|\n","+-------------+----------+------+----------+\n","|        Maria|   Finance|  3000|         1|\n","|        Scott|   Finance|  3300|         2|\n","|          Jen|   Finance|  3900|         3|\n","|        Kumar| Marketing|  2000|         1|\n","|         Jeff| Marketing|  3000|         2|\n","|        James|     Sales|  3000|         1|\n","|        James|     Sales|  3000|         1|\n","|       Robert|     Sales|  4100|         2|\n","|         Saif|     Sales|  4100|         2|\n","|      Michael|     Sales|  4600|         3|\n","+-------------+----------+------+----------+\n","\n","+-------------+----------+------+------------+\n","|employee_name|department|salary|percent_rank|\n","+-------------+----------+------+------------+\n","|        Maria|   Finance|  3000|         0.0|\n","|        Scott|   Finance|  3300|         0.5|\n","|          Jen|   Finance|  3900|         1.0|\n","|        Kumar| Marketing|  2000|         0.0|\n","|         Jeff| Marketing|  3000|         1.0|\n","|        James|     Sales|  3000|         0.0|\n","|        James|     Sales|  3000|         0.0|\n","|       Robert|     Sales|  4100|         0.5|\n","|         Saif|     Sales|  4100|         0.5|\n","|      Michael|     Sales|  4600|         1.0|\n","+-------------+----------+------+------------+\n","\n","+-------------+----------+------+-----+\n","|employee_name|department|salary|ntile|\n","+-------------+----------+------+-----+\n","|        Maria|   Finance|  3000|    1|\n","|        Scott|   Finance|  3300|    1|\n","|          Jen|   Finance|  3900|    2|\n","|        Kumar| Marketing|  2000|    1|\n","|         Jeff| Marketing|  3000|    2|\n","|        James|     Sales|  3000|    1|\n","|        James|     Sales|  3000|    1|\n","|       Robert|     Sales|  4100|    1|\n","|         Saif|     Sales|  4100|    2|\n","|      Michael|     Sales|  4600|    2|\n","+-------------+----------+------+-----+\n","\n","+-------------+----------+------+------------------+\n","|employee_name|department|salary|         cume_dist|\n","+-------------+----------+------+------------------+\n","|        Maria|   Finance|  3000|0.3333333333333333|\n","|        Scott|   Finance|  3300|0.6666666666666666|\n","|          Jen|   Finance|  3900|               1.0|\n","|        Kumar| Marketing|  2000|               0.5|\n","|         Jeff| Marketing|  3000|               1.0|\n","|        James|     Sales|  3000|               0.4|\n","|        James|     Sales|  3000|               0.4|\n","|       Robert|     Sales|  4100|               0.8|\n","|         Saif|     Sales|  4100|               0.8|\n","|      Michael|     Sales|  4600|               1.0|\n","+-------------+----------+------+------------------+\n","\n","+-------------+----------+------+----+\n","|employee_name|department|salary| lag|\n","+-------------+----------+------+----+\n","|        Maria|   Finance|  3000|null|\n","|        Scott|   Finance|  3300|null|\n","|          Jen|   Finance|  3900|3000|\n","|        Kumar| Marketing|  2000|null|\n","|         Jeff| Marketing|  3000|null|\n","|        James|     Sales|  3000|null|\n","|        James|     Sales|  3000|null|\n","|       Robert|     Sales|  4100|3000|\n","|         Saif|     Sales|  4100|3000|\n","|      Michael|     Sales|  4600|4100|\n","+-------------+----------+------+----+\n","\n","+-------------+----------+------+----+\n","|employee_name|department|salary|lead|\n","+-------------+----------+------+----+\n","|        Maria|   Finance|  3000|3900|\n","|        Scott|   Finance|  3300|null|\n","|          Jen|   Finance|  3900|null|\n","|        Kumar| Marketing|  2000|null|\n","|         Jeff| Marketing|  3000|null|\n","|        James|     Sales|  3000|4100|\n","|        James|     Sales|  3000|4100|\n","|       Robert|     Sales|  4100|4600|\n","|         Saif|     Sales|  4100|null|\n","|      Michael|     Sales|  4600|null|\n","+-------------+----------+------+----+\n","\n","+----------+------+-----+----+----+\n","|department|   avg|  sum| min| max|\n","+----------+------+-----+----+----+\n","|   Finance|3400.0|10200|3000|3900|\n","| Marketing|2500.0| 5000|2000|3000|\n","|     Sales|3760.0|18800|3000|4600|\n","+----------+------+-----+----+----+\n","\n"]}]},{"cell_type":"markdown","source":["#JSON"],"metadata":{"id":"_xVhUJnKWMKF"}},{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession,Row\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","\n","jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n","df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n","df.show(truncate=False)\n","\n","#Convert JSON string column to Map type\n","from pyspark.sql.types import MapType,StringType\n","from pyspark.sql.functions import from_json\n","df2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n","df2.printSchema()\n","df2.show(truncate=False)\n","\n","from pyspark.sql.functions import to_json,col\n","df2.withColumn(\"value\",to_json(col(\"value\"))) \\\n","   .show(truncate=False)\n","\n","from pyspark.sql.functions import json_tuple\n","df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n","    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n","    .show(truncate=False)\n","\n","from pyspark.sql.functions import get_json_object\n","df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n","    .show(truncate=False)\n","\n","from pyspark.sql.functions import schema_of_json,lit\n","schemaStr=spark.range(1) \\\n","    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n","    .collect()[0][0]\n","print(schemaStr)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwGO9X5ZWOmT","executionInfo":{"status":"ok","timestamp":1687491783291,"user_tz":-420,"elapsed":4245,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"e50adf15-0f35-4344-bf2d-134c2e4d293a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+--------------------------------------------------------------------------+\n","|id |value                                                                     |\n","+---+--------------------------------------------------------------------------+\n","|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n","+---+--------------------------------------------------------------------------+\n","\n","root\n"," |-- id: long (nullable = true)\n"," |-- value: map (nullable = true)\n"," |    |-- key: string\n"," |    |-- value: string (valueContainsNull = true)\n","\n","+---+---------------------------------------------------------------------------+\n","|id |value                                                                      |\n","+---+---------------------------------------------------------------------------+\n","|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n","+---+---------------------------------------------------------------------------+\n","\n","+---+----------------------------------------------------------------------------+\n","|id |value                                                                       |\n","+---+----------------------------------------------------------------------------+\n","|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n","+---+----------------------------------------------------------------------------+\n","\n","+---+-------+-----------+-----------+\n","|id |Zipcode|ZipCodeType|City       |\n","+---+-------+-----------+-----------+\n","|1  |704    |STANDARD   |PARC PARQUE|\n","+---+-------+-----------+-----------+\n","\n","+---+-----------+\n","|id |ZipCodeType|\n","+---+-----------+\n","|1  |STANDARD   |\n","+---+-----------+\n","\n","STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n"]}]},{"cell_type":"markdown","source":["# Build In Function\n"],"metadata":{"id":"gj9N1l8t4H9-"}},{"cell_type":"markdown","source":["## When\n"],"metadata":{"id":"PO6hXd3i4Ii-"}},{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","data = [(\"James\",\"M\",60000), (\"Michael\",\"M\",70000),\n","        (\"Robert\",None,400000), (\"Maria\",\"F\",500000),\n","        (\"Jen\",\"\",None)]\n","\n","columns = [\"name\",\"gender\",\"salary\"]\n","df = spark.createDataFrame(data = data, schema = columns)\n","df.show()\n","\n","#Using When otherwise\n","from pyspark.sql.functions import when,col\n","df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n","                                 .when(df.gender == \"F\",\"Female\")\n","                                 .when(df.gender.isNull() ,\"\")\n","                                 .otherwise(df.gender))\n","df2.show()\n","\n","df2=df.select(col(\"*\"),when(df.gender == \"M\",\"Male\")\n","                  .when(df.gender == \"F\",\"Female\")\n","                  .when(df.gender.isNull() ,\"\")\n","                  .otherwise(df.gender).alias(\"new_gender\"))\n","df2.show()\n","# Using SQL Case When\n","from pyspark.sql.functions import expr\n","df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n","           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n","          \"ELSE gender END\"))\n","df3.show()\n","\n","df4 = df.select(col(\"*\"), expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n","           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n","           \"ELSE gender END\").alias(\"new_gender\"))\n","\n","df.createOrReplaceTempView(\"EMP\")\n","spark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" +\n","               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n","              \"ELSE gender END as new_gender from EMP\").show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"EeckgKC98jIN","executionInfo":{"status":"error","timestamp":1687501813752,"user_tz":-420,"elapsed":6,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"5e838e77-7c19-45ba-9814-b66ccc9918ef"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cb2b5f501522>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SparkByExamples.com'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m data = [(\"James\",\"M\",60000), (\"Michael\",\"M\",70000),\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"Robert\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Maria\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         (\"Jen\",\"\",None)]\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"Mp-VTioH8jAE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Expr\n"],"metadata":{"id":"4XBN3Pxt4Ilf"}},{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","\n","from pyspark.sql.functions import expr\n","#Concatenate columns\n","data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")]\n","df=spark.createDataFrame(data).toDF(\"col1\",\"col2\")\n","df.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()\n","\n","#Using CASE WHEN sql expression\n","data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n","columns = [\"name\",\"gender\"]\n","df = spark.createDataFrame(data = data, schema = columns)\n","df2 = df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n","           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n","df2.show()\n","\n","#Add months from a value of another column\n","data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n","df=spark.createDataFrame(data).toDF(\"date\",\"increment\")\n","df.select(df.date,df.increment,\n","     expr(\"add_months(date,increment)\")\n","  .alias(\"inc_date\")).show()\n","\n","# Providing alias using 'as'\n","df.select(df.date,df.increment,\n","     expr(\"\"\"add_months(date,increment) as inc_date\"\"\")\n","  ).show()\n","\n","# Add\n","df.select(df.date,df.increment,\n","     expr(\"increment + 5 as new_increment\")\n","  ).show()\n","\n","# Using cast to convert data types\n","df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n","  .printSchema()\n","\n","#Use expr()  to filter the rows\n","data=[(100,2),(200,3000),(500,500)]\n","df=spark.createDataFrame(data).toDF(\"col1\",\"col2\")\n","df.filter(expr(\"col1 == col2\")).show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6JX7Ahl8sU3","executionInfo":{"status":"ok","timestamp":1687501912192,"user_tz":-420,"elapsed":30002,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"1d2421a6-3c9e-43c4-d18e-d7f9b0b3ab21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+-----+-----------+\n","| col1| col2|       Name|\n","+-----+-----+-----------+\n","|James| Bond| James,Bond|\n","|Scott|Varsa|Scott,Varsa|\n","+-----+-----+-----------+\n","\n","+-------+-------+\n","|   name| gender|\n","+-------+-------+\n","|  James|   Male|\n","|Michael| Female|\n","|    Jen|unknown|\n","+-------+-------+\n","\n","+----------+---------+----------+\n","|      date|increment|  inc_date|\n","+----------+---------+----------+\n","|2019-01-23|        1|2019-02-23|\n","|2019-06-24|        2|2019-08-24|\n","|2019-09-20|        3|2019-12-20|\n","+----------+---------+----------+\n","\n","+----------+---------+----------+\n","|      date|increment|  inc_date|\n","+----------+---------+----------+\n","|2019-01-23|        1|2019-02-23|\n","|2019-06-24|        2|2019-08-24|\n","|2019-09-20|        3|2019-12-20|\n","+----------+---------+----------+\n","\n","+----------+---------+-------------+\n","|      date|increment|new_increment|\n","+----------+---------+-------------+\n","|2019-01-23|        1|            6|\n","|2019-06-24|        2|            7|\n","|2019-09-20|        3|            8|\n","+----------+---------+-------------+\n","\n","root\n"," |-- increment: long (nullable = true)\n"," |-- str_increment: string (nullable = true)\n","\n","+----+----+\n","|col1|col2|\n","+----+----+\n","| 500| 500|\n","+----+----+\n","\n"]}]},{"cell_type":"markdown","source":["##Lit"],"metadata":{"id":"tb4WzX6c4Inp"}},{"cell_type":"code","source":["\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import when\n","\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n","columns= [\"EmpId\",\"Salary\"]\n","df = spark.createDataFrame(data = data, schema = columns)\n","df.printSchema()\n","df.show(truncate=False)\n","\n","from pyspark.sql.functions import col,lit\n","df2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\n","df2.show(truncate=False)\n","\n","df3 = df2.withColumn(\"lit_value2\", when(col(\"Salary\") >=40000 & col(\"Salary\") <= 50000,lit(\"100\")).otherwise(lit(\"200\")))\n","df3.show(truncate=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":883},"id":"mKQrqY7B8xwd","executionInfo":{"status":"error","timestamp":1687501962178,"user_tz":-420,"elapsed":1164,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"07b2aa0c-3a97-47d7-c8db-5ec5ec6ed651"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- EmpId: string (nullable = true)\n"," |-- Salary: long (nullable = true)\n","\n","+-----+------+\n","|EmpId|Salary|\n","+-----+------+\n","|111  |50000 |\n","|222  |60000 |\n","|333  |40000 |\n","+-----+------+\n","\n","+-----+------+----------+\n","|EmpId|Salary|lit_value1|\n","+-----+------+----------+\n","|111  |50000 |1         |\n","|222  |60000 |1         |\n","|333  |40000 |1         |\n","+-----+------+----------+\n","\n"]},{"output_type":"error","ename":"Py4JError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-9b8abe17106b>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lit_value2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Salary\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m\u001b[0;36m40000\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Salary\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"200\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    159\u001b[0m     ) -> \"Column\":\n\u001b[1;32m    160\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mnjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 raise Py4JError(\n\u001b[0m\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n","\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o259.and. Trace:\npy4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n"]}]},{"cell_type":"markdown","source":["##Split"],"metadata":{"id":"FDL6UEiJ4Ip9"}},{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder \\\n","         .appName('SparkByExamples.com') \\\n","         .getOrCreate()\n","\n","data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n","            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n","            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n","            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n","            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n","            ]\n","\n","columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n","df=spark.createDataFrame(data,columns)\n","df.printSchema()\n","df.show(truncate=False)\n","\n","from pyspark.sql.functions import split, col\n","df2 = df.select(split(col(\"name\"),\",\").alias(\"NameArray\")) \\\n","    .drop(\"name\")\n","df2.printSchema()\n","df2.show()\n","\n","df.createOrReplaceTempView(\"PERSON\")\n","spark.sql(\"select SPLIT(name,',') as NameArray from PERSON\") \\\n","    .show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3-hkZ_t83e7","executionInfo":{"status":"ok","timestamp":1687501914430,"user_tz":-420,"elapsed":2250,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"2864b3b5-3475-4caf-a83e-a6c1cea46a04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- name: string (nullable = true)\n"," |-- dob_year: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: long (nullable = true)\n","\n","+--------------------+--------+------+------+\n","|name                |dob_year|gender|salary|\n","+--------------------+--------+------+------+\n","|James, A, Smith     |2018    |M     |3000  |\n","|Michael, Rose, Jones|2010    |M     |4000  |\n","|Robert,K,Williams   |2010    |M     |4000  |\n","|Maria,Anne,Jones    |2005    |F     |4000  |\n","|Jen,Mary,Brown      |2010    |      |-1    |\n","+--------------------+--------+------+------+\n","\n","root\n"," |-- NameArray: array (nullable = true)\n"," |    |-- element: string (containsNull = false)\n","\n","+--------------------+\n","|           NameArray|\n","+--------------------+\n","| [James,  A,  Smith]|\n","|[Michael,  Rose, ...|\n","|[Robert, K, Willi...|\n","|[Maria, Anne, Jones]|\n","|  [Jen, Mary, Brown]|\n","+--------------------+\n","\n","+--------------------+\n","|           NameArray|\n","+--------------------+\n","| [James,  A,  Smith]|\n","|[Michael,  Rose, ...|\n","|[Robert, K, Willi...|\n","|[Maria, Anne, Jones]|\n","|  [Jen, Mary, Brown]|\n","+--------------------+\n","\n"]}]},{"cell_type":"markdown","source":["##Concat_WS"],"metadata":{"id":"rg66YZSj4eP9"}},{"cell_type":"code","source":["\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","\n","columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n","data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n","    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n","    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n","\n","df = spark.createDataFrame(data=data,schema=columns)\n","df.printSchema()\n","df.show(truncate=False)\n","\n","from pyspark.sql.functions import col, concat_ws\n","df2 = df.withColumn(\"languagesAtSchool\",\n","   concat_ws(\",\",col(\"languagesAtSchool\")))\n","df2.printSchema()\n","df2.show(truncate=False)\n","\n","df.createOrReplaceTempView(\"ARRAY_STRING\")\n","spark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,\" + \\\n","    \" currentState from ARRAY_STRING\") \\\n","    .show(truncate=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwHYZ5oJ8-HM","executionInfo":{"status":"ok","timestamp":1687501926747,"user_tz":-420,"elapsed":2204,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"ed28cfcd-190f-4b7b-c1a2-0bc632b2724c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- name: string (nullable = true)\n"," |-- languagesAtSchool: array (nullable = true)\n"," |    |-- element: string (containsNull = true)\n"," |-- currentState: string (nullable = true)\n","\n","+----------------+------------------+------------+\n","|name            |languagesAtSchool |currentState|\n","+----------------+------------------+------------+\n","|James,,Smith    |[Java, Scala, C++]|CA          |\n","|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n","|Robert,,Williams|[CSharp, VB]      |NV          |\n","+----------------+------------------+------------+\n","\n","root\n"," |-- name: string (nullable = true)\n"," |-- languagesAtSchool: string (nullable = false)\n"," |-- currentState: string (nullable = true)\n","\n","+----------------+-----------------+------------+\n","|name            |languagesAtSchool|currentState|\n","+----------------+-----------------+------------+\n","|James,,Smith    |Java,Scala,C++   |CA          |\n","|Michael,Rose,   |Spark,Java,C++   |NJ          |\n","|Robert,,Williams|CSharp,VB        |NV          |\n","+----------------+-----------------+------------+\n","\n","+----------------+-----------------+------------+\n","|name            |languagesAtSchool|currentState|\n","+----------------+-----------------+------------+\n","|James,,Smith    |Java,Scala,C++   |CA          |\n","|Michael,Rose,   |Spark,Java,C++   |NJ          |\n","|Robert,,Williams|CSharp,VB        |NV          |\n","+----------------+-----------------+------------+\n","\n"]}]},{"cell_type":"markdown","source":["##Substring"],"metadata":{"id":"B4kjnQf-4eSV"}},{"cell_type":"code","source":["\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, substring\n","spark=SparkSession.builder.appName(\"stringoperations\").getOrCreate()\n","data = [(1,\"20200828\"),(2,\"20180525\")]\n","columns=[\"id\",\"date\"]\n","df=spark.createDataFrame(data,columns)\n","\n","#Using SQL function substring()\n","df.withColumn('year', substring('date', 1,4))\\\n","    .withColumn('month', substring('date', 5,2))\\\n","    .withColumn('day', substring('date', 7,2))\n","df.printSchema()\n","df.show(truncate=False)\n","\n","#Using select\n","df1=df.select('date', substring('date', 1,4).alias('year'), \\\n","                  substring('date', 5,2).alias('month'), \\\n","                  substring('date', 7,2).alias('day'))\n","\n","#Using with selectExpr\n","df2=df.selectExpr('date', 'substring(date, 1,4) as year', \\\n","                  'substring(date, 5,2) as month', \\\n","                  'substring(date, 7,2) as day')\n","\n","#Using substr from Column type\n","df3=df.withColumn('year', col('date').substr(1, 4))\\\n","  .withColumn('month',col('date').substr(5, 2))\\\n","  .withColumn('day', col('date').substr(7, 2))\n","\n","df3.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ThQ3P7D9OUe","executionInfo":{"status":"ok","timestamp":1687501994736,"user_tz":-420,"elapsed":2612,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"f551275f-597b-4744-df2d-cf73c34795f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- id: long (nullable = true)\n"," |-- date: string (nullable = true)\n","\n","+---+--------+\n","|id |date    |\n","+---+--------+\n","|1  |20200828|\n","|2  |20180525|\n","+---+--------+\n","\n","+---+--------+----+-----+---+\n","| id|    date|year|month|day|\n","+---+--------+----+-----+---+\n","|  1|20200828|2020|   08| 28|\n","|  2|20180525|2018|   05| 25|\n","+---+--------+----+-----+---+\n","\n"]}]},{"cell_type":"markdown","source":["##Translate"],"metadata":{"id":"WsIdmQ334eUu"}},{"cell_type":"code","source":[],"metadata":{"id":"pCB5N9hr9U8Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Regex_replace"],"metadata":{"id":"Sc81GdSt4eWz"}},{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n","\n","address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n","    (2,\"43421 Margarita St\",\"NY\"),\n","    (3,\"13111 Siemon Ave\",\"CA\")]\n","\n","df =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n","df.show()\n","\n","#Replace string\n","from pyspark.sql.functions import regexp_replace\n","df.withColumn('address', regexp_replace('address', 'Rd', 'Road')) \\\n","  .show(truncate=False)\n","\n","#Replace string\n","from pyspark.sql.functions import when\n","df.withColumn('address',\n","      when(df.address.endswith('Rd'),regexp_replace(df.address,'Rd','Road')) \\\n","     .when(df.address.endswith('St'),regexp_replace(df.address,'St','Street')) \\\n","     .when(df.address.endswith('Ave'),regexp_replace(df.address,'Ave','Avenue')) \\\n","     .otherwise(df.address)) \\\n","     .show(truncate=False)\n","\n","\n","#Replace values from Dictionary\n","stateDic={'CA':'California','NY':'New York','DE':'Delaware'}\n","df2=df.rdd.map(lambda x:\n","    (x.id,x.address,stateDic[x.state])\n","    ).toDF([\"id\",\"address\",\"state\"])\n","df2.show()\n","\n","#Using translate\n","from pyspark.sql.functions import translate\n","df.withColumn('address', translate('address', '123', 'ABC')) \\\n","  .show(truncate=False)\n","\n","#Replace column with another column\n","from pyspark.sql.functions import expr\n","df = spark.createDataFrame([(\"ABCDE_XYZ\", \"XYZ\",\"FGH\")], (\"col1\", \"col2\",\"col3\"))\n","df.withColumn(\"new_column\",\n","              expr(\"regexp_replace(col1, col2, col3)\")\n","              .alias(\"replaced_value\")\n","              ).show()\n","\n","#Overlay\n","from pyspark.sql.functions import overlay\n","df = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\n","df.select(overlay(\"col1\", \"col2\", 7).alias(\"overlayed\")).show()\n"],"metadata":{"id":"2bZH6lrQ9lGO","executionInfo":{"status":"ok","timestamp":1687502088098,"user_tz":-420,"elapsed":3781,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"8d252100-aae1-416f-cd49-8adcf381a8b8","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+-----+\n","| id|           address|state|\n","+---+------------------+-----+\n","|  1|  14851 Jeffrey Rd|   DE|\n","|  2|43421 Margarita St|   NY|\n","|  3|  13111 Siemon Ave|   CA|\n","+---+------------------+-----+\n","\n","+---+------------------+-----+\n","|id |address           |state|\n","+---+------------------+-----+\n","|1  |14851 Jeffrey Road|DE   |\n","|2  |43421 Margarita St|NY   |\n","|3  |13111 Siemon Ave  |CA   |\n","+---+------------------+-----+\n","\n","+---+----------------------+-----+\n","|id |address               |state|\n","+---+----------------------+-----+\n","|1  |14851 Jeffrey Road    |DE   |\n","|2  |43421 Margarita Street|NY   |\n","|3  |13111 Siemon Avenue   |CA   |\n","+---+----------------------+-----+\n","\n","+---+------------------+----------+\n","| id|           address|     state|\n","+---+------------------+----------+\n","|  1|  14851 Jeffrey Rd|  Delaware|\n","|  2|43421 Margarita St|  New York|\n","|  3|  13111 Siemon Ave|California|\n","+---+------------------+----------+\n","\n","+---+------------------+-----+\n","|id |address           |state|\n","+---+------------------+-----+\n","|1  |A485A Jeffrey Rd  |DE   |\n","|2  |4C4BA Margarita St|NY   |\n","|3  |ACAAA Siemon Ave  |CA   |\n","+---+------------------+-----+\n","\n","+---------+----+----+----------+\n","|     col1|col2|col3|new_column|\n","+---------+----+----+----------+\n","|ABCDE_XYZ| XYZ| FGH| ABCDE_FGH|\n","+---------+----+----+----------+\n","\n","+---------+\n","|overlayed|\n","+---------+\n","|ABCDE_FGH|\n","+---------+\n","\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"0B3JVqRLPEkJ"}},{"cell_type":"markdown","source":["##Overlay"],"metadata":{"id":"sH2EDGtJ417z"}},{"cell_type":"code","source":["\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n","\n","address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n","    (2,\"43421 Margarita St\",\"NY\"),\n","    (3,\"13111 Siemon Ave\",\"CA\")]\n","\n","df =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n","df.show()\n","\n","#Replace string\n","from pyspark.sql.functions import regexp_replace\n","df.withColumn('address', regexp_replace('address', 'Rd', 'Road')) \\\n","  .show(truncate=False)\n","\n","#Replace string\n","from pyspark.sql.functions import when\n","df.withColumn('address',\n","      when(df.address.endswith('Rd'),regexp_replace(df.address,'Rd','Road')) \\\n","     .when(df.address.endswith('St'),regexp_replace(df.address,'St','Street')) \\\n","     .when(df.address.endswith('Ave'),regexp_replace(df.address,'Ave','Avenue')) \\\n","     .otherwise(df.address)) \\\n","     .show(truncate=False)\n","\n","\n","#Replace values from Dictionary\n","stateDic={'CA':'California','NY':'New York','DE':'Delaware'}\n","df2=df.rdd.map(lambda x:\n","    (x.id,x.address,stateDic[x.state])\n","    ).toDF([\"id\",\"address\",\"state\"])\n","df2.show()\n","\n","#Using translate\n","from pyspark.sql.functions import translate\n","df.withColumn('address', translate('address', '123', 'ABC')) \\\n","  .show(truncate=False)\n","\n","#Replace column with another column\n","from pyspark.sql.functions import expr\n","df = spark.createDataFrame([(\"ABCDE_XYZ\", \"XYZ\",\"FGH\")], (\"col1\", \"col2\",\"col3\"))\n","df.withColumn(\"new_column\",\n","              expr(\"regexp_replace(col1, col2, col3)\")\n","              .alias(\"replaced_value\")\n","              ).show()\n","\n","#Overlay\n","from pyspark.sql.functions import overlay\n","df = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\n","df.select(overlay(\"col1\", \"col2\", 7).alias(\"overlayed\")).show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQw1VT_E9aOH","executionInfo":{"status":"ok","timestamp":1687502046690,"user_tz":-420,"elapsed":7058,"user":{"displayName":"3B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"e47be400-f99a-407f-d90a-893e0ad1d40b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+-----+\n","| id|           address|state|\n","+---+------------------+-----+\n","|  1|  14851 Jeffrey Rd|   DE|\n","|  2|43421 Margarita St|   NY|\n","|  3|  13111 Siemon Ave|   CA|\n","+---+------------------+-----+\n","\n","+---+------------------+-----+\n","|id |address           |state|\n","+---+------------------+-----+\n","|1  |14851 Jeffrey Road|DE   |\n","|2  |43421 Margarita St|NY   |\n","|3  |13111 Siemon Ave  |CA   |\n","+---+------------------+-----+\n","\n","+---+----------------------+-----+\n","|id |address               |state|\n","+---+----------------------+-----+\n","|1  |14851 Jeffrey Road    |DE   |\n","|2  |43421 Margarita Street|NY   |\n","|3  |13111 Siemon Avenue   |CA   |\n","+---+----------------------+-----+\n","\n","+---+------------------+----------+\n","| id|           address|     state|\n","+---+------------------+----------+\n","|  1|  14851 Jeffrey Rd|  Delaware|\n","|  2|43421 Margarita St|  New York|\n","|  3|  13111 Siemon Ave|California|\n","+---+------------------+----------+\n","\n","+---+------------------+-----+\n","|id |address           |state|\n","+---+------------------+-----+\n","|1  |A485A Jeffrey Rd  |DE   |\n","|2  |4C4BA Margarita St|NY   |\n","|3  |ACAAA Siemon Ave  |CA   |\n","+---+------------------+-----+\n","\n","+---------+----+----+----------+\n","|     col1|col2|col3|new_column|\n","+---------+----+----+----------+\n","|ABCDE_XYZ| XYZ| FGH| ABCDE_FGH|\n","+---------+----+----+----------+\n","\n","+---------+\n","|overlayed|\n","+---------+\n","|ABCDE_FGH|\n","+---------+\n","\n"]}]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"rrLRqfVZ41_L"}}]}