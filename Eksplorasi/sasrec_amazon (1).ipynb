{"cells":[{"cell_type":"markdown","metadata":{"id":"nTmYV2myfKMB"},"source":["# SASRec & SSEPT\n","\n","### Sequential Recommendation Using Transformer \\[1, 6\\]\n","\n","![image.png](attachment:image.png)\n","\n","This is a class of sequential recommendation that uses Transformer \\[2\\] for encoding the users preference represented in terms of a sequence of items purchased/viewed before. Instead of using CNN (Caser \\[3\\]) or RNN (GRU4Rec \\[4\\], SLI-Rec \\[5\\] etc.) the approach relies on Transformer based encoder that generates a new representation of the item sequence. Two variants of this Transformer based approaches are included here,\n","\n","- Self-Attentive Sequential Recommendation (or SASRec [1]) that is based on vanilla Transformer and models only the item sequence and\n","- Stochastic Shared Embedding based Personalized Transformer or SSE-PT [6], that also models the users along with the items.\n","\n","This notebook provides an example of necessary steps to train and test either a SASRec or a SSE-PT model."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"fcy3-J32fKMD","executionInfo":{"status":"ok","timestamp":1692762187633,"user_tz":-420,"elapsed":3,"user":{"displayName":"4B_Eben Ezer Napitu","userId":"02247100063817301472"}}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","source":["!pip install scrapbook"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dDfzAq5iB97","executionInfo":{"status":"ok","timestamp":1692762922864,"user_tz":-420,"elapsed":5177,"user":{"displayName":"4B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"9678c02b-875c-4eb6-d793-b42fbdca470b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scrapbook in /usr/local/lib/python3.10/dist-packages (0.5.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from scrapbook) (1.5.3)\n","Requirement already satisfied: papermill in /usr/local/lib/python3.10/dist-packages (from scrapbook) (2.4.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from scrapbook) (4.19.0)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from scrapbook) (7.34.0)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from scrapbook) (9.0.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (67.7.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.19.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (4.8.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (0.9.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (2023.3)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (1.23.5)\n","Requirement already satisfied: ansiwrap in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (0.8.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (8.1.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (6.0.1)\n","Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (5.9.2)\n","Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (0.8.0)\n","Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (2.31.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (0.4)\n","Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (8.2.3)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->scrapbook) (0.8.3)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (6.1.12)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (5.3.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (2.18.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->scrapbook) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->scrapbook) (0.2.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->scrapbook) (1.16.0)\n","Requirement already satisfied: textwrap3>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from ansiwrap->papermill->scrapbook) (0.9.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (2023.7.22)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill->scrapbook) (23.2.1)\n","Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill->scrapbook) (6.3.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill->scrapbook) (3.10.0)\n"]}]},{"cell_type":"code","source":["!pip install recommenders"],"metadata":{"id":"CY01eAR6iMzC","executionInfo":{"status":"ok","timestamp":1692762959802,"user_tz":-420,"elapsed":1974,"user":{"displayName":"4B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"4c496a7e-528a-4a15-af99-1ebeed2156bc","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Ignored the following versions that require a different python version: 0.6.0 Requires-Python >=3.6, <=3.8; 0.7.0 Requires-Python >=3.6, <3.8; 1.0.0 Requires-Python >=3.6, <3.9; 1.1.0 Requires-Python >=3.6, <3.10; 1.1.1 Requires-Python >=3.6, <3.10\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement recommenders (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for recommenders\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"NbmeqYDwfKMD","executionInfo":{"status":"error","timestamp":1692762929827,"user_tz":-420,"elapsed":4058,"user":{"displayName":"4B_Eben Ezer Napitu","userId":"02247100063817301472"}},"outputId":"a674a8f1-5e6a-42ae-fbdc-db3e376f8fb8"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-c798b68b6c7d>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only show error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrecommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrecommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamazon_reviews\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_review_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrecommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilter_k_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'recommenders'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import re\n","import sys\n","import os\n","import scrapbook as sb\n","from tempfile import TemporaryDirectory\n","import numpy as np\n","import pandas as pd\n","\n","from collections import defaultdict\n","import tensorflow as tf\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","\n","from recommenders.utils.timer import Timer\n","from recommenders.datasets.amazon_reviews import get_review_data\n","from recommenders.datasets.split_utils import filter_k_core\n","\n","# Transformer Based Models\n","from recommenders.models.sasrec.model import SASREC\n","from recommenders.models.sasrec.ssept import SSEPT\n","\n","# Sampler for sequential prediction\n","from recommenders.models.sasrec.sampler import WarpSampler\n","from recommenders.models.sasrec.util import SASRecDataSet\n","\n","print(\"System version: {}\".format(sys.version))\n","print(\"Tensorflow version: {}\".format(tf.__version__))"]},{"cell_type":"markdown","metadata":{"id":"qLYWYV6QfKME"},"source":["### Input Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["parameters"],"id":"r5b2wiDkfKME"},"outputs":[],"source":["num_epochs = 5\n","batch_size = 128\n","RANDOM_SEED = 100  # Set None for non-deterministic result\n","\n","# data_dir = os.path.join(\"tests\", \"recsys_data\", \"RecSys\", \"SASRec-tf2\", \"data\")\n","data_dir = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"sasrec\")\n","\n","# Amazon Electronics Data\n","dataset = \"reviews_Electronics_5\"\n","\n","lr = 0.001             # learning rate\n","maxlen = 50            # maximum sequence length for each user\n","num_blocks = 2         # number of transformer blocks\n","hidden_units = 100     # number of units in the attention calculation\n","num_heads = 1          # number of attention heads\n","dropout_rate = 0.1     # dropout rate\n","l2_emb = 0.0           # L2 regularization coefficient\n","num_neg_test = 100     # number of negative examples per positive example\n","model_name = 'ssept'  # 'sasrec' or 'ssept'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MWj0KzXfKMF"},"outputs":[],"source":["reviews_name = dataset + '.json'\n","outfile = dataset + '.txt'\n","\n","reviews_file = os.path.join(data_dir, reviews_name)\n","if not os.path.exists(reviews_file):\n","    reviews_output = get_review_data(reviews_file)\n","else:\n","    reviews_output = os.path.join(data_dir, dataset+\".json_output\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weNOgryPfKMF","outputId":"7992d157-7cbc-46db-f525-8512ec9e4e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original: 192403 users and 63001 items\n","Final: 20247 users and 11589 items\n"]}],"source":["if not os.path.exists(os.path.join(data_dir, outfile)):\n","    df = pd.read_csv(reviews_output, sep=\"\\t\", names=[\"userID\", \"itemID\", \"time\"])\n","    df = filter_k_core(df, 10)  # filter for users & items with less than 10 interactions\n","\n","    user_set, item_set = set(df['userID'].unique()), set(df['itemID'].unique())\n","    user_map = dict()\n","    item_map = dict()\n","    for u, user in enumerate(user_set):\n","        user_map[user] = u+1\n","    for i, item in enumerate(item_set):\n","        item_map[item] = i+1\n","\n","    df[\"userID\"] = df[\"userID\"].apply(lambda x: user_map[x])\n","    df[\"itemID\"] = df[\"itemID\"].apply(lambda x: item_map[x])\n","    df = df.sort_values(by=[\"userID\", \"time\"])\n","    df.drop(columns=[\"time\"], inplace=True)\n","    df.to_csv(os.path.join(data_dir, outfile), sep=\"\\t\", header=False, index=False)"]},{"cell_type":"markdown","metadata":{"id":"-p1mSuLifKMF"},"source":["SASRec requires sequence input and sequence target. Targets are for both positive and negative examples. Inputs to the model are\n","\n","* user's item history as input to the transformer\n","* user's item history shifted (by 1) as target to the transformer (positive examples)\n","* a sequence of items that are not equal to the positive examples (negative examples)\n","\n","From each user's history three samples are created. If there are $N_u$ items for user-$u$ then $N_u-2$ items are used in training and the last two items are used for validation and testing, respectively.\n","\n","## Dataset Format\n","\n","- The input files should have the following format:\n","    - each row has user-id and item-id converted into integers (starting from 1)\n","    - the rows are sorted by user-id and time of interaction\n","    - for every user the last item is used for testing and the last but one is used for validation\n","    - for example, for user `30449` the sorted inputs are:\n","        - `30449 2771`\n","        - `30449 61842`\n","        - `30449 60293`\n","        - `30449 30047`\n","        - `30449 63296`\n","        - `30449 22042`\n","        - `30449 6717`\n","        - `30449 75780`\n","      \n","      then the train inputs are\n","        - [`2771`, `61842`, `60293`, `30047`, `63296`] (input sequence)\n","        - [`61842`, `60293`, `30047`, `63296`, `22042`] (target sequence for positive examples)\n","        - [`1001`, `50490`, `33312`, `19294`, `45342`] (sample negative examples)\n","\n","      and the validation inputs are\n","        - [`2771`, `61842`, `60293`, `30047`, `63296`, `22042`] (input sequence)\n","        - [`61842`, `60293`, `30047`, `63296`, `22042`, `6717`] (target sequence for positive examples)\n","        - [`4401`, `60351`, `22176`, `23456`, `45342`, '1193`] (sample negative examples)\n","        \n","      and the test inputs are\n","        - [`2771`, `61842`, `60293`, `30047`, `63296`, `22042`, `6717`] (input sequence)\n","        - [`61842`, `60293`, `30047`, `63296`, `22042`, `6717`, `75780`] (target sequence for positive examples)\n","        - [`4401`, `60351`, `22176`, `23456`, `45342`, '1193`, `54231`] (sample negative examples)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvbTt5urfKMF","outputId":"0885f467-d721-49d0-e548-ffdbf5fb06c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["../../tests/resources/deeprec/sasrec/reviews_Electronics_5.txt\n","20247 Users and 11589 items\n","average sequence length: 15.16\n"]}],"source":["inp_file = os.path.join(data_dir, dataset + \".txt\")\n","print(inp_file)\n","\n","# initiate a dataset class\n","data = SASRecDataSet(filename=inp_file, col_sep=\"\\t\")\n","\n","# create train, validation and test splits\n","data.split()\n","\n","# some statistics\n","num_steps = int(len(data.user_train) / batch_size)\n","cc = 0.0\n","for u in data.user_train:\n","    cc += len(data.user_train[u])\n","print('%g Users and %g items' % (data.usernum, data.itemnum))\n","print('average sequence length: %.2f' % (cc / len(data.user_train)))"]},{"cell_type":"markdown","metadata":{"id":"X6iRwUr8fKMG"},"source":["### Model Creation\n","\n","Model parameters are\n","\n","    - number of items\n","    - maximum sequence length of the user interaction history\n","    - number of Transformer blocks\n","    - embedding dimension for item embedding\n","    - dimension of the attention\n","    - number of attention heads\n","    - dropout rate\n","    - dimension of the convolution layers, list\n","    - L_2-regularization coefficient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtXP0cjRfKMG"},"outputs":[],"source":["if model_name == 'sasrec':\n","    model = SASREC(item_num=data.itemnum,\n","                   seq_max_len=maxlen,\n","                   num_blocks=num_blocks,\n","                   embedding_dim=hidden_units,\n","                   attention_dim=hidden_units,\n","                   attention_num_heads=num_heads,\n","                   dropout_rate=dropout_rate,\n","                   conv_dims = [100, 100],\n","                   l2_reg=l2_emb,\n","                   num_neg_test=num_neg_test\n","    )\n","elif model_name == \"ssept\":\n","    model = SSEPT(item_num=data.itemnum,\n","                  user_num=data.usernum,\n","                  seq_max_len=maxlen,\n","                  num_blocks=num_blocks,\n","                  # embedding_dim=hidden_units,  # optional\n","                  user_embedding_dim=10,\n","                  item_embedding_dim=hidden_units,\n","                  attention_dim=hidden_units,\n","                  attention_num_heads=num_heads,\n","                  dropout_rate=dropout_rate,\n","                  conv_dims = [110, 110],\n","                  l2_reg=l2_emb,\n","                  num_neg_test=num_neg_test\n","    )\n","else:\n","    print(f\"Model-{model_name} not found\")"]},{"cell_type":"markdown","metadata":{"id":"3ErwjowofKMG"},"source":["### Sampler\n","\n","    - the sampler creates negative samples from the training data for each batch\n","    - this is done by looking at the original user interaction history and creating items that are not present at all\n","    - the sampler generates a sequence of negative items of the same length as the original history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Or_49UcWfKMG"},"outputs":[],"source":["sampler = WarpSampler(data.user_train, data.usernum, data.itemnum, batch_size=batch_size, maxlen=maxlen, n_workers=3)"]},{"cell_type":"markdown","metadata":{"id":"x1DCG8KufKMG"},"source":["### Model Training\n","\n","    - the loss function is defined over all the negative and positive logits\n","    - a mask has to be applied to indicate the non-zero items present in the output\n","    - we also add the regularization loss here\n","    \n","    - having a train-step signature function can speed up the training process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpMegoIWfKMG","outputId":"165dc957-a2f7-48a8-b6cb-307eace5f3b0"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                      "]},{"name":"stdout","output_type":"stream","text":["\n","epoch: 5, test (NDCG@10: 0.3099896446332482, HR@10: 0.5142)\n","Time cost for training is 7.17 mins\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["with Timer() as train_time:\n","    t_test = model.train(data, sampler, num_epochs=num_epochs, batch_size=batch_size, lr=lr, val_epoch=6)\n","\n","print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bj0g7AaofKMG","outputId":"bc741a37-d0a3-4930-a058-16029351288c"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'ndcg@10': 0.3037326157112286, 'Hit@10': 0.5036}\n"]}],"source":["res_syn = {\"ndcg@10\": t_test[0], \"Hit@10\": t_test[1]}\n","print(res_syn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLnVcUgIfKMH","outputId":"3728e376-b9fc-49ae-ea37-e069e9963174"},"outputs":[{"data":{"application/scrapbook.scrap.json+json":{"data":0.3410638795485906,"encoder":"json","name":"ndcg@10","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"ndcg@10"}},"output_type":"display_data"},{"data":{"application/scrapbook.scrap.json+json":{"data":0.5421,"encoder":"json","name":"Hit@10","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"Hit@10"}},"output_type":"display_data"}],"source":["# Record results with papermill for tests - ignore this cell\n","# sb.glue(\"res_syn\", res_syn)\n","\n","sb.glue(\"ndcg@10\", t_test[0])\n","sb.glue(\"Hit@10\", t_test[1])"]},{"cell_type":"markdown","metadata":{"id":"_lw1phG8fKMH"},"source":["## Reference\n","\\[1\\] Wang-Cheng Kang, Julian McAuley: Self-Attentive Sequential Recommendation, arXiv preprint arXiv:1808.09781 (2018) <br>\n","\n","\\[2\\] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. 5998–6008 <br>\n","\n","\\[3\\] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 565–573.\n","\n","\\[4\\] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015)\n","\n","\\[5\\] Zeping Yu, Jianxun Lian, Ahmad Mahmoody, Gongshen Liu, Xing Xie. Adaptive User Modeling with Long and Short-Term Preferences for Personailzed Recommendation. In Proceedings of the 28th International Joint Conferences on Artificial Intelligence, IJCAI’19, Pages 4213-4219. AAAI Press, 2019.\n","\n","\\[6\\] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, James Sharpnack. SSE-PT: Sequential Recommendation Via Personalized Transformer. In Fourteenth ACM Conference on Recommender Systems, RecSys'20:, Pages 328–337, 2020."]}],"metadata":{"celltoolbar":"Tags","interpreter":{"hash":"adf311e09e3d70e4b770d653e66a69805c21f44d471e9851e226c4ddc6ad9826"},"kernelspec":{"display_name":"reco_gpu","language":"python","name":"reco_gpu"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}